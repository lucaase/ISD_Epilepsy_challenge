{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Import the packages"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before running your first cell, make sure GPU is enabled! Click the three dots in the upper right, go to 'Accelerator' and select 'GPU P100'"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T17:22:28.682185Z","iopub.status.busy":"2023-08-10T17:22:28.681657Z","iopub.status.idle":"2023-08-10T17:22:37.172917Z","shell.execute_reply":"2023-08-10T17:22:37.171699Z","shell.execute_reply.started":"2023-08-10T17:22:28.682153Z"},"trusted":true},"outputs":[],"source":["import time\n","import torch\n","import librosa\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F \n","import matplotlib.pyplot as plt\n","from torch import nn\n","from torchmetrics import F1Score\n","from collections import OrderedDict\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","from inception import Inception, InceptionBlock\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Load and preprocess the training data"]},{"cell_type":"code","execution_count":28,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-08-10T17:22:45.028653Z","iopub.status.busy":"2023-08-10T17:22:45.027652Z","iopub.status.idle":"2023-08-10T17:22:45.103717Z","shell.execute_reply":"2023-08-10T17:22:45.102523Z","shell.execute_reply.started":"2023-08-10T17:22:45.028599Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(3, 2, 240000)\n","(3, 2, 60000)\n","(2, 720000)\n","(2, 180000)\n"]},{"data":{"text/plain":["' # further reshape to 1x144000 matrix (1 electrode, 1440 seconds of data)\\nbasal_train_data = basal_train_reshaped.reshape((1, -1))\\npre_seizure_train_data = pre_seizure_train_reshaped.reshape((1, -1))\\n\\nprint(basal_train_data.shape)\\nprint(pre_seizure_train_data.shape)  '"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# Loading the data files from Kaggle into this workspace\n","basal_train = np.load('basal_train.npy')\n","pre_seizure_train = np.load('pre_seizure_train.npy')\n","\n","# Checking the dimensions of our data\n","print(basal_train.shape)\n","print(pre_seizure_train.shape)\n","\n","# Original train data is 3x2x24000, 3 subjects, 2 electrodes and 240 seconds\n","# Below we will concatenate across subjects\n","\n","# Reshape basal_train to a 2x72000 matrix (2 electrodes, 720 seconds of data)\n","basal_train_reshaped = basal_train.reshape((basal_train.shape[1], -1))\n","pre_seizure_train_reshaped = pre_seizure_train.reshape((pre_seizure_train.shape[1], -1))\n","\n","# Check the dimensions of basal_train_reshaped\n","print(basal_train_reshaped.shape)\n","print(pre_seizure_train_reshaped.shape)\n","\n","\n","\"\"\" # further reshape to 1x144000 matrix (1 electrode, 1440 seconds of data)\n","basal_train_data = basal_train_reshaped.reshape((1, -1))\n","pre_seizure_train_data = pre_seizure_train_reshaped.reshape((1, -1))\n","\n","print(basal_train_data.shape)\n","print(pre_seizure_train_data.shape)  \"\"\"\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Slice data into 2s segments"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T17:23:08.849727Z","iopub.status.busy":"2023-08-10T17:23:08.848936Z","iopub.status.idle":"2023-08-10T17:23:08.857887Z","shell.execute_reply":"2023-08-10T17:23:08.856328Z","shell.execute_reply.started":"2023-08-10T17:23:08.849685Z"},"trusted":true},"outputs":[],"source":["# Create a function to receive longer segments of data and divide it into smaller blocks\n","\n","# The function receives the data variable, the size of the smaller blocks it will be divided into\n","# and the related training labels (which tell whether that block is pre-epileptic or not)\n","def create_windows_per_recording(data, window_size, label):\n","    # Initialize the variables\n","    windows = []\n","    labels = []\n","    \n","    # Compute the number of windows based on the length of the data and the size of the window\n","    num_windows = (data.shape[1] - window_size) // window_size + 1\n","    for i in range(num_windows):\n","        # Fill the windows with segments of the original data\n","        window = data[:, i * window_size : i * window_size + window_size]\n","        windows.append(window)\n","        labels.append(label)\n","        \n","    # Return the resulting smaller windows and its labels\n","    return np.array(windows), np.array(labels)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Set parameters and split train-test data\n"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T17:23:15.690446Z","iopub.status.busy":"2023-08-10T17:23:15.689713Z","iopub.status.idle":"2023-08-10T17:23:15.711994Z","shell.execute_reply":"2023-08-10T17:23:15.710746Z","shell.execute_reply.started":"2023-08-10T17:23:15.690406Z"},"trusted":true},"outputs":[],"source":["# Parameters\n","window_size = 2000  # equivalent to two seconds of data, equal to the test samples\n","\n","# Create windows\n","# Assign '0' to the basal (non pre-epileptic) data and '1' to pre-epileptic\n","basal_windows, basal_labels = create_windows_per_recording(basal_train_reshaped, window_size, 0) \n","pre_seizure_windows, pre_seizure_labels = create_windows_per_recording(pre_seizure_train_reshaped, window_size, 1)\n","\n","# Concatenate the training windows and their labels\n","X_train = np.concatenate([basal_windows, pre_seizure_windows])\n","y_train = np.concatenate([basal_labels, pre_seizure_labels])\n","# remove the singleton dimension\n","X_train = np.squeeze(X_train)\n","y_train = np.squeeze(y_train)\n","\n","##### Data augmentations #####\n","\n","# Noise\n","noise = np.random.normal(0, 0.05, X_train.shape)\n","aug_noise = X_train + noise\n","\n","# Scaling\n","scaling_factor = np.random.uniform(0.5, 1.5)\n","aug_scale = X_train * scaling_factor\n","\n","\"\"\" # Slicing\n","start_index = np.random.randint(0, X_train.shape[1] - window_size)\n","aug_slice = X_train[:, start_index:start_index + window_size] \"\"\"\n","\n","# Flipping\n","aug_flipped = X_train[:, ::-1]\n","\n","# Append augmented data to original training dataset\n","X_train = np.concatenate([X_train, aug_noise, aug_scale, aug_flipped])\n","y_train = np.concatenate([y_train, y_train, y_train, y_train])\n","\n","# Split data into training and validation sets \n","# 'test_size' specifies % of data towards validation set, 0.2 = 20%\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","# 'random_state' ensures reproducibility of this split"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape:  (1440, 2, 2000)\n","y_train shape:  (1440,)\n","X_val shape:  (360, 2, 2000)\n","y_val shape:  (360,)\n"]}],"source":["# print the shapes of the data\n","print(\"X_train shape: \", X_train.shape)\n","print(\"y_train shape: \", y_train.shape)\n","print(\"X_val shape: \", X_val.shape)\n","print(\"y_val shape: \", y_val.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Test spectrogram conversion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_val.shape)\n","print(y_val.shape)\n","\n","X_train_reshaped = X_train.transpose(0, 2, 1).reshape(-1, 2000)\n","X_val_reshaped = X_val.transpose(0, 2, 1).reshape(-1, 2000)\n","print(X_train_reshaped.shape)\n","print(X_val_reshaped.shape)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def data_to_spectrogram(data):\n","    # Initialize a list to store spectrograms\n","    spectrograms = []\n","    # Loop through each EEG sample and compute its STFT\n","    for sample in data:\n","        # Convert the data sample to a frequency domain representation using the STFT\n","        spectrogram = librosa.stft(data, n_fft=1024)\n","        # Normalize the spectrogram\n","        spectrogram = librosa.amplitude_to_db(spectrogram)\n","        spectrograms.append(spectrogram)\n","        \n","    return spectrograms\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the list of spectrograms to a numpy array\n","spectrograms_train = torch.tensor(np.array(data_to_spectrogram(X_train_reshaped)))\n","spectrograms_val = torch.tensor(np.array(data_to_spectrogram(X_val_reshaped)))\n","\n","# Convert the spectrogram to a PyTorch tensor\n","#spectrogram = torch.tensor(spectrogram)\n","\n","print(spectrograms_train.shape)\n","print(spectrograms_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.signal import spectrogram, welch\n","\n","srate = 1000\n","window_length = 0.5*srate\n","overlap = 0.9*window_length\n","nfft = 2**13\n","\n","# normalize this spectrogram in decibels\n","def normalize_spectrogram(Sxx):\n","    Sxx = 10*np.log10(Sxx)\n","    Sxx = Sxx - np.max(Sxx)\n","    return Sxx\n","\n","F,T,Sxx = spectrogram(X_train_reshaped[0],srate,nperseg=int(window_length),noverlap=overlap,nfft=nfft)\n","\n","normalized_Sxx = normalize_spectrogram(Sxx)\n","\n","plt.figure(figsize=(12,6))\n","plt.pcolormesh(T,F,Sxx)\n","plt.ylim([0,25])\n","plt.xlabel('Time [s]')\n","plt.ylabel('Frequency [Hz]')\n","plt.colorbar(label='Power');"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Build the model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## LSTM model"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T17:23:33.192471Z","iopub.status.busy":"2023-08-10T17:23:33.191757Z","iopub.status.idle":"2023-08-10T17:23:37.064077Z","shell.execute_reply":"2023-08-10T17:23:37.062863Z","shell.execute_reply.started":"2023-08-10T17:23:33.192432Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["LSTM(\n","  (lstm): LSTM(2000, 100, num_layers=500, batch_first=True)\n","  (fc): Linear(in_features=100, out_features=1, bias=True)\n",")\n"]}],"source":["# Current model is an LSTM (Long Short Term Memory) using PyTorch\n","\n","class LSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n","        super(LSTM, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","        # Define the LSTM layer\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n","\n","        # Define the output layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        # Initialize hidden state\n","        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device) \n","\n","        # Initialize cell state\n","        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","\n","        return out\n","\n","# Initialize model parameters\n","# 'input_dim' = number of nodes in the first layer of the model, has to be equal to training input data\n","# 'output_dim' = number of nodes in the last layer of the model, will output model prediction\n","# 'n_layers' = number of hidden layers, meaning layers between input and output \n","# 'hidden_dim' = number of nodes in each hidden layer, 100 = layer can store 100 units of information at each time step\n","\n","\n","model = LSTM(input_dim=2000, hidden_dim=100, output_dim=1, n_layers=500)\n","print(model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## InceptionTime model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Preprocessing functions"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class Flatten(nn.Module):\n","\tdef __init__(self, out_features):\n","\t\tsuper(Flatten, self).__init__()\n","\t\tself.output_dim = out_features\n","\n","\tdef forward(self, x):\n","\t\treturn x.view(-1, self.output_dim)\n","    \n","class Reshape(nn.Module):\n","\tdef __init__(self, out_shape):\n","\t\tsuper(Reshape, self).__init__()\n","\t\tself.out_shape = out_shape\n","\n","\tdef forward(self, x):\n","\t\treturn x.view(-1, *self.out_shape)"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["# Scaling the data \n","scaler = RobustScaler()\n","X_train = X_train.reshape(-1, X_train.shape[-1])\n","X_train = scaler.fit_transform(X_train)\n","X_train = X_train.reshape(-1, 2, 2000)\n","X_val = X_val.reshape(-1, X_val.shape[-1])\n","X_val = scaler.transform(X_val)\n","X_val = X_val.reshape(-1, 2, 2000)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["InceptionTime = nn.Sequential(\n","                    Reshape(out_shape=(2,2000)),\n","                    InceptionBlock(\n","                        in_channels=2, \n","                        n_filters=32, \n","                        kernel_sizes=[5, 11, 23],\n","                        bottleneck_channels=32,\n","                        use_residual=True,\n","                        activation=nn.ReLU()\n","                    ),\n","                    nn.Dropout(0.2),\n","                    InceptionBlock(\n","                        in_channels=32*4, \n","                        n_filters=32, \n","                        kernel_sizes=[5, 11, 23],\n","                        bottleneck_channels=32,\n","                        use_residual=True,\n","                        activation=nn.ReLU()\n","                    ),\n","                    nn.Dropout(0.2),\n","                    nn.AdaptiveAvgPool1d(output_size=1),\n","                    Flatten(out_features=32*4*1),\n","                    nn.Linear(in_features=4*32*1, out_features=4)\n","        )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## CNN model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Transform the data into Pytorch datasets and loaders"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T17:23:40.388765Z","iopub.status.busy":"2023-08-10T17:23:40.388311Z","iopub.status.idle":"2023-08-10T17:23:40.419256Z","shell.execute_reply":"2023-08-10T17:23:40.418231Z","shell.execute_reply.started":"2023-08-10T17:23:40.388722Z"},"trusted":true},"outputs":[],"source":["# Convert data to PyTorch tensors (a data structure)\n","tensor_x_train = torch.Tensor(X_train) \n","tensor_x_val = torch.Tensor(X_val) \n","tensor_y_train = torch.Tensor(y_train).long()\n","tensor_y_val = torch.Tensor(y_val).long()\n","\n","# Create Tensor datasets\n","train_data = TensorDataset(tensor_x_train, tensor_y_train)\n","val_data = TensorDataset(tensor_x_val, tensor_y_val)\n","\n","# Dataloaders (to feed the data into the model during training)\n","# 'batch_size' defines the chunk size of data to be fed in each training step\n","batch_size = 128\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n","val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T15:20:00.269612Z","iopub.status.busy":"2023-08-10T15:20:00.269047Z","iopub.status.idle":"2023-08-10T15:20:00.277689Z","shell.execute_reply":"2023-08-10T15:20:00.276456Z","shell.execute_reply.started":"2023-08-10T15:20:00.269574Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([360, 2, 2000]) torch.Size([360])\n","torch.Size([1440, 2, 2000]) torch.Size([1440])\n"]}],"source":["print(tensor_x_val.shape,tensor_y_val.shape)\n","print(tensor_x_train.shape,tensor_y_train.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Check if the dataloader is functioning properly"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T17:23:46.032415Z","iopub.status.busy":"2023-08-10T17:23:46.031691Z","iopub.status.idle":"2023-08-10T17:23:46.065044Z","shell.execute_reply":"2023-08-10T17:23:46.064038Z","shell.execute_reply.started":"2023-08-10T17:23:46.032372Z"},"trusted":true},"outputs":[],"source":["train_iter = iter(train_loader)\n","try:\n","    data, label = next(train_iter)\n","except StopIteration:\n","    print(\"The train_loader is empty.\")\n","    \n","val_iter = iter(val_loader)\n","try:\n","    data, label = next(val_iter)\n","except StopIteration:\n","    print(\"The val_loader is empty.\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T17:23:48.773491Z","iopub.status.busy":"2023-08-10T17:23:48.773102Z","iopub.status.idle":"2023-08-10T17:37:55.031264Z","shell.execute_reply":"2023-08-10T17:37:55.030094Z","shell.execute_reply.started":"2023-08-10T17:23:48.773456Z"},"trusted":true},"outputs":[],"source":["# LSTM model\n","\n","# Loss and optimization\n","# 'device' sets which computing unit will be used, GPU(cuda) is preferred\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# 'criterion' specifies the method to compute loss (an 'error' metric for model prediction)\n","# Here we use BCE (Binary Cross Entropy)\n","criterion = nn.BCEWithLogitsLoss().to(device)\n","# 'optimizer' sets the algorithm that is used to adjust the weights of the model\n","# Here we use ADAM, which is a fairly popular optimization algo\n","# 'lr' means Learning Rate, which modulates how fast the weights are adjusted\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","# .to(device) simply means whe are directing this to the GPU or CPU (chosen above)\n","model = model.to(device)\n","\n","# Initialize training losses, validation losses and F1 scores\n","train_losses = []\n","val_losses = []\n","val_f1_scores = []\n","\n","\n","####### Training loop #######\n","\n","# 'F1Score' is the evaluation metric we use to assess model performance\n","# simple 'accuracy' is the most popular option, but has its tradeoffs\n","f1 = F1Score(task=\"binary\").to(device)\n","\n","# epoch is each training step\n","for epoch in range(100):\n","    # initialize single evaluation metrics (for this iteration)\n","    loss = None\n","    val_loss = None\n","    val_f1 = None\n","    \n","    # Train the model\n","    model.train()\n","    # receive inputs and labels from the training dataloader\n","    for inputs, labels in train_loader:\n","        # send inputs and labels to device\n","        inputs, labels = inputs.to(device), labels.to(device) \n","        # clear out the gradients of all parameters that the optimizer is tracking\n","        optimizer.zero_grad()\n","        # make model predictions\n","        outputs = model(inputs)\n","        # compute training loss based on predicted values\n","        loss = criterion(outputs, labels)\n","        # perform back propagation\n","        loss.backward()\n","        # apply optimizer to adjust the weights\n","        optimizer.step()\n","    \n","    # compute model performance metrics\n","    model.eval()\n","    with torch.no_grad():\n","        # receive inputs and labels from the validation dataloader\n","        for inputs, labels in val_loader:\n","            # send inputs and labels to device\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            # make model predictions\n","            outputs = model(inputs)\n","            # compute training loss and F1Score based on predicted values\n","            val_loss = criterion(outputs, labels)\n","            val_f1 = f1(torch.sigmoid(outputs).round(), labels)\n","            \n","    # add this iteration evaluation metrics to the grouping variables        \n","    train_losses.append(loss.item())\n","    val_losses.append(val_loss.item())\n","    val_f1_scores.append(val_f1.item())\n","    \n","    # Print the loss and F1Score values across epochs\n","    if loss is not None and val_loss is not None and val_f1 is not None:\n","        print(f'Epoch {epoch+1}/{10}, Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val F1 Score: {val_f1.item()}')\n","    else:\n","        print('No data in the DataLoader')\n"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Training Loss: 0.4341347434303977, Validation Loss: 0.4634779989719391\n","Epoch 2, Training Loss: 0.3792206807570024, Validation Loss: 0.4703887850046158\n","Epoch 3, Training Loss: 0.371783817356283, Validation Loss: 0.44111527502536774\n","Epoch 4, Training Loss: 0.36367538571357727, Validation Loss: 0.4252285808324814\n","Epoch 5, Training Loss: 0.35461818359114905, Validation Loss: 0.44830740988254547\n","Epoch 6, Training Loss: 0.35579553788358514, Validation Loss: 0.4538297802209854\n","Epoch 7, Training Loss: 0.3261983801018108, Validation Loss: 0.3990340977907181\n","Epoch 8, Training Loss: 0.3199315206571059, Validation Loss: 0.4102412760257721\n","Epoch 9, Training Loss: 0.29404588314619934, Validation Loss: 0.39466118812561035\n","Epoch 10, Training Loss: 0.3005551899021322, Validation Loss: 0.3847735971212387\n","Epoch 11, Training Loss: 0.2911912460218776, Validation Loss: 0.3782476782798767\n","Epoch 12, Training Loss: 0.2943935868414966, Validation Loss: 0.4297523945569992\n","Epoch 13, Training Loss: 0.3140518936243924, Validation Loss: 0.41397517919540405\n","Epoch 14, Training Loss: 0.2908186113292521, Validation Loss: 0.33704228699207306\n","Epoch 15, Training Loss: 0.26906048032370483, Validation Loss: 0.31690414249897003\n","Epoch 16, Training Loss: 0.2592022987929257, Validation Loss: 0.34316110610961914\n","Epoch 17, Training Loss: 0.2584788487716155, Validation Loss: 0.2944284677505493\n","Epoch 18, Training Loss: 0.22932863370938736, Validation Loss: 0.30391135811805725\n","Epoch 19, Training Loss: 0.22336849163879047, Validation Loss: 0.28853806108236313\n","Epoch 20, Training Loss: 0.19963404129851947, Validation Loss: 0.3334266245365143\n","Epoch 21, Training Loss: 0.20886305110021072, Validation Loss: 0.3183615803718567\n","Epoch 22, Training Loss: 0.23526361042802985, Validation Loss: 0.3124871551990509\n","Epoch 23, Training Loss: 0.2307810282165354, Validation Loss: 0.23958559334278107\n","Epoch 24, Training Loss: 0.18946133418516678, Validation Loss: 0.2770378589630127\n","Epoch 25, Training Loss: 0.23749036274173044, Validation Loss: 0.3894960731267929\n","Epoch 26, Training Loss: 0.22864241085269235, Validation Loss: 0.3041784465312958\n","Epoch 27, Training Loss: 0.1921465816822919, Validation Loss: 0.2561344727873802\n","Epoch 28, Training Loss: 0.17181462049484253, Validation Loss: 0.2498457357287407\n","Epoch 29, Training Loss: 0.17085961320183493, Validation Loss: 0.2703123390674591\n","Epoch 30, Training Loss: 0.18498250300234015, Validation Loss: 0.24656541645526886\n","Epoch 31, Training Loss: 0.157866815274412, Validation Loss: 0.2886221781373024\n","Epoch 32, Training Loss: 0.15382108363238248, Validation Loss: 0.24307610094547272\n","Epoch 33, Training Loss: 0.14404950697313657, Validation Loss: 0.19977300614118576\n","Epoch 34, Training Loss: 0.13096219978549264, Validation Loss: 0.2618468552827835\n","Epoch 35, Training Loss: 0.1105554300275716, Validation Loss: 0.20109501481056213\n","Epoch 36, Training Loss: 0.09441352500156923, Validation Loss: 0.18317408114671707\n","Epoch 37, Training Loss: 0.08861301771619103, Validation Loss: 0.20873578637838364\n","Epoch 38, Training Loss: 0.10109891336072575, Validation Loss: 0.18202047049999237\n","Epoch 39, Training Loss: 0.10444522310387004, Validation Loss: 0.24933813512325287\n","Epoch 40, Training Loss: 0.11199932274493304, Validation Loss: 0.23126626759767532\n","Epoch 41, Training Loss: 0.0946772416884249, Validation Loss: 0.20109343528747559\n","Epoch 42, Training Loss: 0.09434005143967542, Validation Loss: 0.16979727894067764\n","Epoch 43, Training Loss: 0.07693830775943669, Validation Loss: 0.16092826426029205\n","Epoch 44, Training Loss: 0.07477153905413368, Validation Loss: 0.18957580626010895\n","Epoch 45, Training Loss: 0.0680462961847132, Validation Loss: 0.17021673172712326\n","Epoch 46, Training Loss: 0.04933086850426414, Validation Loss: 0.12399905174970627\n","Epoch 47, Training Loss: 0.05010853402993896, Validation Loss: 0.12875402718782425\n","Epoch 48, Training Loss: 0.04550346749072725, Validation Loss: 0.10349206998944283\n","Epoch 49, Training Loss: 0.07596339386972514, Validation Loss: 0.2530900910496712\n","Epoch 50, Training Loss: 0.13185429674657909, Validation Loss: 0.6077500283718109\n","Epoch 51, Training Loss: 0.13443005830049515, Validation Loss: 0.24123939871788025\n","Epoch 52, Training Loss: 0.12184930321845142, Validation Loss: 0.14677509665489197\n","Epoch 53, Training Loss: 0.08508448235013268, Validation Loss: 0.19282017648220062\n","Epoch 54, Training Loss: 0.0745771388438615, Validation Loss: 0.10208820924162865\n","Epoch 55, Training Loss: 0.06332304362546314, Validation Loss: 0.13916438072919846\n","Epoch 56, Training Loss: 0.052987350658936935, Validation Loss: 0.1231384426355362\n","Epoch 57, Training Loss: 0.04105626944113861, Validation Loss: 0.11421677842736244\n","Epoch 58, Training Loss: 0.03297976624559273, Validation Loss: 0.1018356904387474\n","Epoch 59, Training Loss: 0.03484684618359262, Validation Loss: 0.07312489673495293\n","Epoch 60, Training Loss: 0.03596554137766361, Validation Loss: 0.10867349430918694\n","Epoch 61, Training Loss: 0.040859197012402794, Validation Loss: 0.11852143704891205\n","Epoch 62, Training Loss: 0.033342535184188324, Validation Loss: 0.08195561915636063\n","Epoch 63, Training Loss: 0.0265037805180658, Validation Loss: 0.07154841721057892\n","Epoch 64, Training Loss: 0.023840755224227905, Validation Loss: 0.07466340065002441\n","Epoch 65, Training Loss: 0.026482768864794212, Validation Loss: 0.058583300560712814\n","Epoch 66, Training Loss: 0.025803927501494236, Validation Loss: 0.06531075201928616\n","Epoch 67, Training Loss: 0.028268837454644116, Validation Loss: 0.10326307266950607\n","Epoch 68, Training Loss: 0.03411414423449473, Validation Loss: 0.08537316508591175\n","Epoch 69, Training Loss: 0.038140970705585045, Validation Loss: 0.0792485810816288\n","Epoch 70, Training Loss: 0.054536996239965614, Validation Loss: 0.12456156313419342\n","Epoch 71, Training Loss: 0.0962096205489202, Validation Loss: 0.22037266194820404\n","Epoch 72, Training Loss: 0.10430954261259599, Validation Loss: 0.1786172240972519\n","Epoch 73, Training Loss: 0.08254357664422556, Validation Loss: 0.14155949652194977\n","Epoch 74, Training Loss: 0.07555889744650233, Validation Loss: 0.12128586322069168\n","Epoch 75, Training Loss: 0.049479725008661095, Validation Loss: 0.09459288790822029\n","Epoch 76, Training Loss: 0.04550542022016915, Validation Loss: 0.09223385900259018\n","Epoch 77, Training Loss: 0.04248765195635232, Validation Loss: 0.09233066812157631\n","Epoch 78, Training Loss: 0.03571736287664284, Validation Loss: 0.08773599565029144\n","Epoch 79, Training Loss: 0.030499195341359486, Validation Loss: 0.07705311849713326\n","Epoch 80, Training Loss: 0.025910115885463627, Validation Loss: 0.06810036674141884\n","Epoch 81, Training Loss: 0.022036797113039276, Validation Loss: 0.06494089402258396\n","Epoch 82, Training Loss: 0.02030235173350031, Validation Loss: 0.05770319886505604\n","Epoch 83, Training Loss: 0.02009010094810616, Validation Loss: 0.056900737807154655\n","Epoch 84, Training Loss: 0.032202511856501755, Validation Loss: 0.08339830860495567\n","Epoch 85, Training Loss: 0.035745900801636955, Validation Loss: 0.10421793535351753\n","Epoch 86, Training Loss: 0.04304621199315244, Validation Loss: 0.09929310157895088\n","Epoch 87, Training Loss: 0.04511171748692339, Validation Loss: 0.12608051672577858\n","Epoch 88, Training Loss: 0.05014594305645336, Validation Loss: 0.09884507581591606\n","Epoch 89, Training Loss: 0.04053392091935331, Validation Loss: 0.07509099505841732\n","Epoch 90, Training Loss: 0.04064458981156349, Validation Loss: 0.07293181866407394\n","Epoch 91, Training Loss: 0.04547982104122639, Validation Loss: 0.08964404463768005\n","Epoch 92, Training Loss: 0.03380095027387142, Validation Loss: 0.06332308240234852\n","Epoch 93, Training Loss: 0.029631370847875423, Validation Loss: 0.06924052722752094\n","Epoch 94, Training Loss: 0.027266452088952065, Validation Loss: 0.06473230570554733\n","Epoch 95, Training Loss: 0.022168207066980274, Validation Loss: 0.07457218877971172\n","Epoch 96, Training Loss: 0.021075524559075184, Validation Loss: 0.0732140950858593\n","Epoch 97, Training Loss: 0.01970210468227213, Validation Loss: 0.05096769519150257\n","Epoch 98, Training Loss: 0.018270323899659244, Validation Loss: 0.05955353006720543\n","Epoch 99, Training Loss: 0.01749141717498953, Validation Loss: 0.0502189751714468\n","Epoch 100, Training Loss: 0.01922621112316847, Validation Loss: 0.053383443504571915\n","Epoch 101, Training Loss: 0.03309849158606746, Validation Loss: 0.08588574081659317\n","Epoch 102, Training Loss: 0.05421533909710971, Validation Loss: 0.12282049655914307\n","Epoch 103, Training Loss: 0.07705241238529031, Validation Loss: 0.1020934171974659\n","Epoch 104, Training Loss: 0.09138257225806062, Validation Loss: 0.13450630754232407\n","Epoch 105, Training Loss: 0.08344450051134283, Validation Loss: 0.15329895168542862\n","Epoch 106, Training Loss: 0.06368384611877528, Validation Loss: 0.08089790306985378\n","Epoch 107, Training Loss: 0.04845544645054774, Validation Loss: 0.08479398861527443\n","Epoch 108, Training Loss: 0.03317288529466499, Validation Loss: 0.07939820736646652\n","Epoch 109, Training Loss: 0.03511859679763967, Validation Loss: 0.09820389375090599\n","Epoch 110, Training Loss: 0.03347544947808439, Validation Loss: 0.06454726308584213\n","Epoch 111, Training Loss: 0.027826921167698773, Validation Loss: 0.07996068149805069\n","Epoch 112, Training Loss: 0.025442861359227787, Validation Loss: 0.08084367588162422\n","Epoch 113, Training Loss: 0.020397442139007828, Validation Loss: 0.0510743148624897\n","Epoch 114, Training Loss: 0.018407146869735283, Validation Loss: 0.03946380503475666\n","Epoch 115, Training Loss: 0.016027445650913498, Validation Loss: 0.049097515642642975\n","Epoch 116, Training Loss: 0.01681624192067168, Validation Loss: 0.057707661762833595\n","Epoch 117, Training Loss: 0.017392145131121983, Validation Loss: 0.07386256381869316\n","Epoch 118, Training Loss: 0.02844442410225218, Validation Loss: 0.08522279560565948\n","Epoch 119, Training Loss: 0.03036524270745841, Validation Loss: 0.058412808924913406\n","Epoch 120, Training Loss: 0.02277583950622515, Validation Loss: 0.09581038355827332\n","Epoch 121, Training Loss: 0.024204151874238796, Validation Loss: 0.0749431885778904\n","Epoch 122, Training Loss: 0.019592195833948525, Validation Loss: 0.0474861953407526\n","Epoch 123, Training Loss: 0.019514886154369873, Validation Loss: 0.04699011147022247\n","Epoch 124, Training Loss: 0.02111315947364677, Validation Loss: 0.05845687352120876\n","Epoch 125, Training Loss: 0.03498391519215974, Validation Loss: 0.0885419026017189\n","Epoch 126, Training Loss: 0.04223475232720375, Validation Loss: 0.07049800083041191\n","Epoch 127, Training Loss: 0.04994009756906466, Validation Loss: 0.09544931724667549\n","Epoch 128, Training Loss: 0.1237313984469934, Validation Loss: 1.133364498615265\n","Epoch 129, Training Loss: 0.4110760715874759, Validation Loss: 0.4310295134782791\n","Epoch 130, Training Loss: 0.3736091608350927, Validation Loss: 0.48831045627593994\n","Epoch 131, Training Loss: 0.33701179921627045, Validation Loss: 0.4198065847158432\n","Epoch 132, Training Loss: 0.3016450378027829, Validation Loss: 0.3819562494754791\n","Epoch 133, Training Loss: 0.2827286733822389, Validation Loss: 0.3199737071990967\n","Epoch 134, Training Loss: 0.27091835845600476, Validation Loss: 0.28721244633197784\n","Epoch 135, Training Loss: 0.24074217270721088, Validation Loss: 0.3331538736820221\n","Epoch 136, Training Loss: 0.22455997494134036, Validation Loss: 0.3024031072854996\n","Epoch 137, Training Loss: 0.19966737790541214, Validation Loss: 0.28994741290807724\n","Epoch 138, Training Loss: 0.1903092007745396, Validation Loss: 0.23278185725212097\n","Epoch 139, Training Loss: 0.16690527850931342, Validation Loss: 0.20255066454410553\n","Epoch 140, Training Loss: 0.14787899093194443, Validation Loss: 0.21834466606378555\n","Epoch 141, Training Loss: 0.1460737795992331, Validation Loss: 0.23009639978408813\n","Epoch 142, Training Loss: 0.11497653885321184, Validation Loss: 0.16599702090024948\n","Epoch 143, Training Loss: 0.10394467142495242, Validation Loss: 0.14897911995649338\n","Epoch 144, Training Loss: 0.09266559779644012, Validation Loss: 0.17964451760053635\n","Epoch 145, Training Loss: 0.09770738604393872, Validation Loss: 0.1594853289425373\n","Epoch 146, Training Loss: 0.08202164755626158, Validation Loss: 0.14255061000585556\n","Epoch 147, Training Loss: 0.07360720803791826, Validation Loss: 0.1549191139638424\n","Epoch 148, Training Loss: 0.07045086507092822, Validation Loss: 0.11460549384355545\n","Epoch 149, Training Loss: 0.06367466293952682, Validation Loss: 0.11701081693172455\n","Epoch 150, Training Loss: 0.04922328178178181, Validation Loss: 0.10899017751216888\n","Epoch 151, Training Loss: 0.03873614391142672, Validation Loss: 0.10895813256502151\n","Epoch 152, Training Loss: 0.033341369506987656, Validation Loss: 0.07484563812613487\n","Epoch 153, Training Loss: 0.03873495368117636, Validation Loss: 0.11236399412155151\n","Epoch 154, Training Loss: 0.043336703018708664, Validation Loss: 0.09811767563223839\n","Epoch 155, Training Loss: 0.040151974694295364, Validation Loss: 0.06460149213671684\n","Epoch 156, Training Loss: 0.037856981666250664, Validation Loss: 0.13202444091439247\n","Epoch 157, Training Loss: 0.050812572918154976, Validation Loss: 0.13482031226158142\n","Epoch 158, Training Loss: 0.0373155821792104, Validation Loss: 0.07603749632835388\n","Epoch 159, Training Loss: 0.03207159736617045, Validation Loss: 0.07865472882986069\n","Epoch 160, Training Loss: 0.027199542488564144, Validation Loss: 0.06533125601708889\n","Epoch 161, Training Loss: 0.02356756292283535, Validation Loss: 0.08960035815834999\n","Epoch 162, Training Loss: 0.02438205616040663, Validation Loss: 0.05327596142888069\n","Epoch 163, Training Loss: 0.02093381197615103, Validation Loss: 0.07031472772359848\n","Epoch 164, Training Loss: 0.02852673667737029, Validation Loss: 0.09345966950058937\n","Epoch 165, Training Loss: 0.03302981789139184, Validation Loss: 0.09673171862959862\n","Epoch 166, Training Loss: 0.025785299356688152, Validation Loss: 0.06886452436447144\n","Epoch 167, Training Loss: 0.022263918071985245, Validation Loss: 0.07434061542153358\n","Epoch 168, Training Loss: 0.01899900668385354, Validation Loss: 0.07445614971220493\n","Epoch 169, Training Loss: 0.01898553188551556, Validation Loss: 0.07111469097435474\n","Epoch 170, Training Loss: 0.02430042539807883, Validation Loss: 0.09960992634296417\n","Epoch 171, Training Loss: 0.04015391150658781, Validation Loss: 0.12582538276910782\n","Epoch 172, Training Loss: 0.04511072994633154, Validation Loss: 0.07921073399484158\n","Epoch 173, Training Loss: 0.036480241709134796, Validation Loss: 0.06784103997051716\n","Epoch 174, Training Loss: 0.04028455117209391, Validation Loss: 0.09595685824751854\n","Epoch 175, Training Loss: 0.0353576254776933, Validation Loss: 0.07107918709516525\n","Epoch 176, Training Loss: 0.029659480025822468, Validation Loss: 0.06551867350935936\n","Epoch 177, Training Loss: 0.03696531497619369, Validation Loss: 0.07322131097316742\n","Epoch 178, Training Loss: 0.03634593906727704, Validation Loss: 0.08350299298763275\n","Epoch 179, Training Loss: 0.02799461680379781, Validation Loss: 0.07069047726690769\n","Epoch 180, Training Loss: 0.02201811495152387, Validation Loss: 0.057106154039502144\n","Epoch 181, Training Loss: 0.018214680000462315, Validation Loss: 0.06520376820117235\n","Epoch 182, Training Loss: 0.015694152055816216, Validation Loss: 0.073778435587883\n","Epoch 183, Training Loss: 0.016402592120522804, Validation Loss: 0.07235996797680855\n","Epoch 184, Training Loss: 0.017646644010462544, Validation Loss: 0.05082012712955475\n","Epoch 185, Training Loss: 0.014971344054422596, Validation Loss: 0.06959138996899128\n","Epoch 186, Training Loss: 0.015093289739028974, Validation Loss: 0.06467057578265667\n","Epoch 187, Training Loss: 0.016865277154879135, Validation Loss: 0.0791937056928873\n","Epoch 188, Training Loss: 0.01711252856660973, Validation Loss: 0.07216491922736168\n","Epoch 189, Training Loss: 0.02113856290551749, Validation Loss: 0.06062573380768299\n","Epoch 190, Training Loss: 0.024238203567537395, Validation Loss: 0.07108623720705509\n","Epoch 191, Training Loss: 0.02218738706274466, Validation Loss: 0.05618655867874622\n","Epoch 192, Training Loss: 0.021536942402070217, Validation Loss: 0.07825805805623531\n","Epoch 193, Training Loss: 0.026462465524673462, Validation Loss: 0.06470668688416481\n","Epoch 194, Training Loss: 0.06127817251465537, Validation Loss: 0.10807636007666588\n","Epoch 195, Training Loss: 0.061285606839440086, Validation Loss: 0.1429452896118164\n","Epoch 196, Training Loss: 0.06048175726424564, Validation Loss: 0.08888285420835018\n","Epoch 197, Training Loss: 0.04958558218045668, Validation Loss: 0.07733013853430748\n","Epoch 198, Training Loss: 0.038603869690136475, Validation Loss: 0.07929091528058052\n","Epoch 199, Training Loss: 0.027044505896893414, Validation Loss: 0.07189741544425488\n","Epoch 200, Training Loss: 0.02517500655217604, Validation Loss: 0.05459810234606266\n","Epoch 201, Training Loss: 0.02081895060837269, Validation Loss: 0.07682131975889206\n","Epoch 202, Training Loss: 0.01877189199016853, Validation Loss: 0.04769997112452984\n","Epoch 203, Training Loss: 0.014858973043208773, Validation Loss: 0.06712976843118668\n","Epoch 204, Training Loss: 0.014948385598307306, Validation Loss: 0.048194797709584236\n","Epoch 205, Training Loss: 0.015714066780426285, Validation Loss: 0.05930593051016331\n","Epoch 206, Training Loss: 0.015180336734787985, Validation Loss: 0.03870016150176525\n","Epoch 207, Training Loss: 0.014183884486556053, Validation Loss: 0.04963571019470692\n","Epoch 208, Training Loss: 0.014578745531087572, Validation Loss: 0.054417094215750694\n","Epoch 209, Training Loss: 0.01664474911310456, Validation Loss: 0.0753622492775321\n","Epoch 210, Training Loss: 0.018427855250510303, Validation Loss: 0.05164242535829544\n","Epoch 211, Training Loss: 0.018881965936584907, Validation Loss: 0.052547845989465714\n","Epoch 212, Training Loss: 0.018734773451631718, Validation Loss: 0.08073243871331215\n","Epoch 213, Training Loss: 0.04474536837501959, Validation Loss: 0.16506794095039368\n","Epoch 214, Training Loss: 0.1897564469413324, Validation Loss: 0.5022988766431808\n","Epoch 215, Training Loss: 0.24262056025591763, Validation Loss: 0.3175786882638931\n","Epoch 216, Training Loss: 0.21383386647159403, Validation Loss: 0.23425781726837158\n","Epoch 217, Training Loss: 0.14869808744300494, Validation Loss: 0.2518055737018585\n","Epoch 218, Training Loss: 0.12016808648001064, Validation Loss: 0.19329305738210678\n","Epoch 219, Training Loss: 0.09668880226937207, Validation Loss: 0.15458685904741287\n","Epoch 220, Training Loss: 0.07964217662811279, Validation Loss: 0.14228586107492447\n","Epoch 221, Training Loss: 0.06295824626630003, Validation Loss: 0.11693564057350159\n","Epoch 222, Training Loss: 0.04393990507180041, Validation Loss: 0.09271687269210815\n","Epoch 223, Training Loss: 0.037354166534813965, Validation Loss: 0.08403200656175613\n","Epoch 224, Training Loss: 0.03534322062676603, Validation Loss: 0.10337568446993828\n","Epoch 225, Training Loss: 0.032595633613792335, Validation Loss: 0.06471115909516811\n","Epoch 226, Training Loss: 0.027765201235359364, Validation Loss: 0.06046201288700104\n","Epoch 227, Training Loss: 0.028084438294172287, Validation Loss: 0.08941681310534477\n","Epoch 228, Training Loss: 0.025807775049047035, Validation Loss: 0.07358656637370586\n","Epoch 229, Training Loss: 0.025466613302176647, Validation Loss: 0.07516126334667206\n","Epoch 230, Training Loss: 0.023369974880055946, Validation Loss: 0.06290804408490658\n","Epoch 231, Training Loss: 0.020253614776513794, Validation Loss: 0.07923978939652443\n","Epoch 232, Training Loss: 0.025596373968503693, Validation Loss: 0.06519016623497009\n","Epoch 233, Training Loss: 0.025713085281577976, Validation Loss: 0.0720287598669529\n","Epoch 234, Training Loss: 0.0236307004974647, Validation Loss: 0.06760481372475624\n","Epoch 235, Training Loss: 0.03168387033722617, Validation Loss: 0.11027314141392708\n","Epoch 236, Training Loss: 0.03162794289263812, Validation Loss: 0.058702973648905754\n","Epoch 237, Training Loss: 0.025364217602393845, Validation Loss: 0.0794560257345438\n","Epoch 238, Training Loss: 0.02479422295635397, Validation Loss: 0.06733401492238045\n","Epoch 239, Training Loss: 0.022894470698454163, Validation Loss: 0.04765359312295914\n","Epoch 240, Training Loss: 0.022465126080946488, Validation Loss: 0.08044910058379173\n","Epoch 241, Training Loss: 0.017188117924061688, Validation Loss: 0.06713009998202324\n","Epoch 242, Training Loss: 0.015849228863689033, Validation Loss: 0.0496960673481226\n","Epoch 243, Training Loss: 0.017418204806745052, Validation Loss: 0.05912860482931137\n","Epoch 244, Training Loss: 0.017742572267624466, Validation Loss: 0.07121407799422741\n","Epoch 245, Training Loss: 0.01712642474607988, Validation Loss: 0.055529145523905754\n","Epoch 246, Training Loss: 0.018259947865524075, Validation Loss: 0.05099285766482353\n","Epoch 247, Training Loss: 0.02218813601542603, Validation Loss: 0.06982453167438507\n","Epoch 248, Training Loss: 0.028318890793757004, Validation Loss: 0.06681129895150661\n","Epoch 249, Training Loss: 0.02486595765433528, Validation Loss: 0.052800726145505905\n","Epoch 250, Training Loss: 0.018985334275798363, Validation Loss: 0.06542863510549068\n","Epoch 251, Training Loss: 0.017179113982075996, Validation Loss: 0.06118658930063248\n","Epoch 252, Training Loss: 0.016593107191676445, Validation Loss: 0.04004264995455742\n","Epoch 253, Training Loss: 0.016717490655454723, Validation Loss: 0.07439866662025452\n","Epoch 254, Training Loss: 0.021727135912938553, Validation Loss: 0.048598963767290115\n","Epoch 255, Training Loss: 0.02976143021475185, Validation Loss: 0.06477714516222477\n","Epoch 256, Training Loss: 0.03578054447743026, Validation Loss: 0.06317662075161934\n","Epoch 257, Training Loss: 0.030259736728939144, Validation Loss: 0.06095165200531483\n","Epoch 258, Training Loss: 0.02300747229971669, Validation Loss: 0.056395379826426506\n","Epoch 259, Training Loss: 0.021362872794270515, Validation Loss: 0.06495879217982292\n","Epoch 260, Training Loss: 0.01789496373385191, Validation Loss: 0.05629056692123413\n","Epoch 261, Training Loss: 0.018059388565068894, Validation Loss: 0.041529517620801926\n","Epoch 262, Training Loss: 0.015597917647524313, Validation Loss: 0.07217466086149216\n","Epoch 263, Training Loss: 0.014551241882145405, Validation Loss: 0.04969303496181965\n","Epoch 264, Training Loss: 0.021008332005955956, Validation Loss: 0.06847639009356499\n","Epoch 265, Training Loss: 0.02677771720019254, Validation Loss: 0.05397450923919678\n","Epoch 266, Training Loss: 0.03332154808396643, Validation Loss: 0.061829015612602234\n","Epoch 267, Training Loss: 0.03567340986972505, Validation Loss: 0.063072195276618\n","Epoch 268, Training Loss: 0.02836400659924204, Validation Loss: 0.07193376123905182\n","Epoch 269, Training Loss: 0.021162748675454746, Validation Loss: 0.06923637166619301\n","Epoch 270, Training Loss: 0.02175450993871147, Validation Loss: 0.06627728790044785\n","Epoch 271, Training Loss: 0.01756313671781258, Validation Loss: 0.03573050536215305\n","Epoch 272, Training Loss: 0.016266514293172142, Validation Loss: 0.03457854688167572\n","Epoch 273, Training Loss: 0.016824947246773678, Validation Loss: 0.059027114883065224\n","Epoch 274, Training Loss: 0.017856174317950554, Validation Loss: 0.04429277777671814\n","Epoch 275, Training Loss: 0.025364501401782036, Validation Loss: 0.06833702325820923\n","Epoch 276, Training Loss: 0.024068565700541843, Validation Loss: 0.04534744471311569\n","Epoch 277, Training Loss: 0.018177149902013214, Validation Loss: 0.04225039482116699\n","Epoch 278, Training Loss: 0.01667165146632628, Validation Loss: 0.045147258788347244\n","Epoch 279, Training Loss: 0.013898544521494345, Validation Loss: 0.035980671644210815\n","Epoch 280, Training Loss: 0.014005185189572248, Validation Loss: 0.03118214476853609\n","Epoch 281, Training Loss: 0.017652356150475414, Validation Loss: 0.0695835780352354\n","Epoch 282, Training Loss: 0.016635384664616802, Validation Loss: 0.05459199100732803\n","Epoch 283, Training Loss: 0.01885523096742955, Validation Loss: 0.041261546313762665\n","Epoch 284, Training Loss: 0.020795096304606308, Validation Loss: 0.05168347805738449\n","Epoch 285, Training Loss: 0.020967468958009373, Validation Loss: 0.05036786198616028\n","Epoch 286, Training Loss: 0.01821763195436109, Validation Loss: 0.037127536721527576\n","Epoch 287, Training Loss: 0.01582222499630668, Validation Loss: 0.04741939529776573\n","Epoch 288, Training Loss: 0.015468657778745348, Validation Loss: 0.0458557503297925\n","Epoch 289, Training Loss: 0.017027106644077736, Validation Loss: 0.0626405319198966\n","Epoch 290, Training Loss: 0.019578172655945473, Validation Loss: 0.06269212998449802\n","Epoch 291, Training Loss: 0.02049136720597744, Validation Loss: 0.06626169569790363\n","Epoch 292, Training Loss: 0.025401439009742302, Validation Loss: 0.05320343188941479\n","Epoch 293, Training Loss: 0.03618513962084597, Validation Loss: 0.08916953206062317\n","Epoch 294, Training Loss: 0.0339970106089657, Validation Loss: 0.05859445407986641\n","Epoch 295, Training Loss: 0.0467024352401495, Validation Loss: 0.09168539568781853\n","Epoch 296, Training Loss: 0.0488695658066056, Validation Loss: 0.09139643236994743\n","Epoch 297, Training Loss: 0.037282796915281906, Validation Loss: 0.11347318068146706\n","Epoch 298, Training Loss: 0.031211083754897118, Validation Loss: 0.06303390860557556\n","Epoch 299, Training Loss: 0.021289706399494953, Validation Loss: 0.0336948623880744\n","Epoch 300, Training Loss: 0.01704700401222164, Validation Loss: 0.043470658361911774\n","Epoch 301, Training Loss: 0.01282666911455718, Validation Loss: 0.03389238752424717\n","Epoch 302, Training Loss: 0.01201613840054382, Validation Loss: 0.05260849557816982\n","Epoch 303, Training Loss: 0.01284964086318558, Validation Loss: 0.03562253061681986\n","Epoch 304, Training Loss: 0.013107616285031492, Validation Loss: 0.04019447136670351\n"]}],"source":["# InceptionTime model\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.Adam(InceptionTime.parameters(), lr=0.001, weight_decay=0.01)\n","InceptionTime = InceptionTime.to(device)\n","\n","\n","train_losses = []\n","val_losses = []\n","\n","min_val_loss = np.inf\n","#patience = 10\n","#patience_counter = 0\n","\n","# Training loop\n","for epoch in range(1000):\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = InceptionTime(inputs)\n","        loss = criterion(outputs, labels.long())\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    # Validation loss computation\n","    val_running_loss = 0.0\n","    for i, data in enumerate(val_loader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        outputs = InceptionTime(inputs)\n","        val_loss = criterion(outputs, labels.long())\n","\n","        val_running_loss += val_loss.item()\n","\n","    train_losses.append(running_loss / len(train_loader))\n","    val_losses.append(val_running_loss / len(val_loader))\n","    \n","    if val_loss < min_val_loss:\n","        # Save the model\n","        torch.save(InceptionTime.state_dict(), 'best_model.pth')\n","        min_val_loss = val_loss\n","        \n","\n","    \n","\n","    print(f'Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader)}, Validation Loss: {val_running_loss / len(val_loader)}')\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["patience_counter = 0  # Reset the patience counter\n","    else:\n","        # Increment the patience counter\n","        patience_counter += 1\n","# Check if we've run out of patience\n","    if patience_counter >= patience:\n","        print(\"Early stopping...\")\n","        break\n","\n","    # Save the model every 50 epochs\n","    if epoch % 50 == 0:\n","        torch.save(InceptionTime.state_dict(), f'InceptionTime_model_epoch_{epoch}.pth')\n","\n","# Save the InceptionTime model\n","\n","# Save the InceptionTime model\n","#torch.save(InceptionTime.state_dict(), 'InceptionTime_model_80%Train.pth')\n","\n","# Load the model from a file\n","#InceptionTime.load_state_dict(torch.load('InceptionTime_model_80%Train.pth'))\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Plotting model performance"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T18:09:18.714849Z","iopub.status.busy":"2023-08-10T18:09:18.714268Z","iopub.status.idle":"2023-08-10T18:09:18.999055Z","shell.execute_reply":"2023-08-10T18:09:18.997996Z","shell.execute_reply.started":"2023-08-10T18:09:18.714796Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Plot the results\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(val_losses, label='Validation loss')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Create submission file using model predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load the test set from kaggle into this workspace\n","test_set = np.load('/kaggle/input/ix-neuroengineering-symposium-epilepsy-challenge/test_set.npy')\n","\n","# Convert to tensor and create DataLoader\n","tensor_x_test = torch.Tensor(test_set)\n","test_data = TensorDataset(tensor_x_test)\n","test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Get model predictions\n","model.eval()\n","predictions = []\n","with torch.no_grad():\n","    for inputs in test_loader:\n","        outputs = model(inputs[0].to(device))\n","        probabilities = torch.sigmoid(outputs).cpu().numpy()  # Convert to numpy array\n","        binary_predictions = (probabilities > 0.5).astype(int)  # Apply threshold\n","        predictions.extend(binary_predictions)\n","\n","# Convert binary predictions to scalar values (0 or 1)\n","predictions = [pred.item() for pred in predictions]\n","\n","# Create submission file\n","with open(\"submission.csv\", \"w\") as f:\n","    f.write(\"win_id,label\\n\")\n","    for i, pred in enumerate(predictions):\n","        f.write(f\"{i},{pred}\\n\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Load the model from a file\n","InceptionTime.load_state_dict(torch.load('best_model.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the test set\n","batch_size = 128\n","\n","test_set = np.load('test_set.npy')\n","\n","# Scaling the data \n","scaler = RobustScaler()\n","X_test = test_set.reshape(-1, test_set.shape[-1])\n","X_test = scaler.fit_transform(X_test)\n","X_test = X_test.reshape(-1, 2, 2000)\n","\n","\n","# Convert to tensor and create DataLoader\n","tensor_x_test = torch.Tensor(test_set)\n","test_data = TensorDataset(tensor_x_test)\n","test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Get model predictions\n","InceptionTime = InceptionTime.to(device)\n","InceptionTime.eval()\n","predictions = []\n","with torch.no_grad():\n","    for inputs in test_loader:\n","        inputs = inputs[0].to(device)\n","        outputs = InceptionTime(inputs)\n","        print(outputs.data)\n","        _, predicted = torch.max(outputs.data, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","\n","# Create submission file\n","with open(\"submission.csv\", \"w\") as f:\n","    f.write(\"win_id,label\\n\")\n","    for i, pred in enumerate(predictions):\n","        f.write(f\"{i},{pred}\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
